# LLM Benchmark for Arm Server (AS)
![supporting image]()

### Project Difficulty
Intermediate
**Audience:** Computer Science (CS) / Electrical Engineering (EE)

## Description
This project aims to create an MLPerf benchmark infrastructure for performance analysis across different configurations of Arm-based servers. The main deliverable is a comprehensive benchmarking setup that can evaluate the performance of large language models (LLMs) on various Arm server configurations. This project will provide practical experience in benchmarking, performance analysis, and working with Arm-based server architectures. The final output will be a detailed report and a functional benchmarking infrastructure that can be used for further research and development.

## Estimated Project Duration
The project is estimated to take 6-8 weeks to complete, involving a team of 3-4 participants. There is no hard deadline, but timely completion is encouraged to maximize learning outcomes.

## Hardware / Software Requirements
- Languages: Python, C++
- Tooling: MLPerf, TensorFlow, PyTorch
- Hardware: Arm-based server, access to cloud service providers
- IP access: Arm Academic Access member (link to get if they don't have it)

## Resources
- Learning paths: Online courses on benchmarking, performance analysis, and large language models
- Textbooks: "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
- Similar projects: Previous benchmarking projects available on GitHub
- Previous project submissions: GitHub link to past projects

## Benefits / Prizes
Participants will gain practical experience in benchmarking, performance analysis, and working with Arm-based server architectures. Outstanding projects may receive kudos and recognition on academic platforms.